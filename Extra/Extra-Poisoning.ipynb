{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Poisoning Attacks against Machine Learning models\n",
    "\n",
    "In this tutorial we will experiment with **adversarial poisoning attacks** against a Support Vector Machine (SVM) with Radial Basis Function (RBF) kernel.\n",
    "\n",
    "Poisoning attacks are performed at *train time* by injecting *carefully crafted samples* that alter the classifier decision function so that its accuracy decreases.\n",
    "\n",
    "As in the previous tutorials, we will first create and train the classifier, evaluating its performance in the standard scenario, *i.e. not under attack*.\n",
    "The poisoning attack will also need a *validation set* to verify the classifier performance during the attack, so we split the training set furtherly in two.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](\n",
    "https://colab.research.google.com/github/zangobot/teaching_material/blob/HEAD/Extra-Poisoning.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%capture --no-stderr --no-display\n",
    "# NBVAL_IGNORE_OUTPUT\n",
    "\n",
    "try:\n",
    "    import secml\n",
    "except ImportError:\n",
    "    %pip install git+https://github.com/pralab/secml\n",
    "    %pip install foolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "random_state = 999\n",
    "\n",
    "n_features = 2  # Number of features\n",
    "n_samples = 300  # Number of samples\n",
    "centers = [[-1, -1], [+1, +1]]  # Centers of the clusters\n",
    "cluster_std = 0.9  # Standard deviation of the clusters\n",
    "\n",
    "from secml.data.loader import CDLRandomBlobs\n",
    "\n",
    "dataset = CDLRandomBlobs(n_features=n_features,\n",
    "                         centers=centers,\n",
    "                         cluster_std=cluster_std,\n",
    "                         n_samples=n_samples,\n",
    "                         random_state=random_state).load()\n",
    "\n",
    "n_tr = 100  # Number of training set samples\n",
    "n_val = 100  # Number of validation set samples\n",
    "n_ts = 100  # Number of test set samples\n",
    "\n",
    "# Split in training, validation and test\n",
    "from secml.data.splitter import CTrainTestSplit\n",
    "\n",
    "splitter = CTrainTestSplit(\n",
    "    train_size=n_tr + n_val, test_size=n_ts, random_state=random_state)\n",
    "tr_val, ts = splitter.split(dataset)\n",
    "splitter = CTrainTestSplit(\n",
    "    train_size=n_tr, test_size=n_val, random_state=random_state)\n",
    "tr, val = splitter.split(dataset)\n",
    "\n",
    "# Normalize the data\n",
    "from secml.ml.features import CNormalizerMinMax\n",
    "\n",
    "nmz = CNormalizerMinMax()\n",
    "tr.X = nmz.fit_transform(tr.X)\n",
    "val.X = nmz.transform(val.X)\n",
    "ts.X = nmz.transform(ts.X)\n",
    "\n",
    "# Metric to use for training and performance evaluation\n",
    "from secml.ml.peval.metrics import CMetricAccuracy\n",
    "\n",
    "metric = CMetricAccuracy()\n",
    "\n",
    "# Creation of the multiclass classifier\n",
    "from secml.ml.classifiers import CClassifierSVM\n",
    "from secml.ml.kernels import CKernelRBF\n",
    "\n",
    "clean_clf = CClassifierSVM(kernel=CKernelRBF(gamma=10), C=1)\n",
    "\n",
    "# We can now fit the classifier\n",
    "clean_clf.fit(tr.X, tr.Y)\n",
    "print(\"Training of classifier complete!\")\n",
    "\n",
    "# Compute predictions on a test set\n",
    "y_pred = clean_clf.predict(ts.X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Generation of Poisoning Samples\n",
    "\n",
    "We are going to generate an adversarial example against the SVM classifier using the **gradient-based** algorithm for generating poisoning attacks proposed in:\n",
    " \n",
    "  > [[biggio12-icml]](https://arxiv.org/abs/1206.6389)\n",
    "  > Biggio, B., Nelson, B. and Laskov, P., 2012. Poisoning attacks against \n",
    "  > support vector machines. In ICML 2012.\n",
    "\n",
    "  > [[biggio15-icml]](https://arxiv.org/abs/1804.07933)\n",
    "  > Xiao, H., Biggio, B., Brown, G., Fumera, G., Eckert, C. and Roli, F., 2015. \n",
    "  > Is feature selection secure against training data poisoning?. In ICML 2015.\n",
    "\n",
    "  > [[demontis19-usenix]](\n",
    "  > https://www.usenix.org/conference/usenixsecurity19/presentation/demontis)\n",
    "  > Demontis, A., Melis, M., Pintor, M., Jagielski, M., Biggio, B., Oprea, A., \n",
    "  > Nita-Rotaru, C. and Roli, F., 2019. Why Do Adversarial Attacks Transfer? \n",
    "  > Explaining Transferability of Evasion and Poisoning Attacks. In 28th Usenix \n",
    "  > Security Symposium, Santa Clara, California, USA.\n",
    "\n",
    "To compute a poisoning point, a bi-level optimization problem has to be solved, namely:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\max_{x_c}& A(D_{val}, \\mathbf{w}^\\ast) = \\sum_{j=1}^m \\ell(y_j, \\mathbf{x_\\mathit{j}}, \\mathbf{w}^\\ast)\\\\\n",
    "&s.t. \\mathbf{w}^\\ast \\in \\underset{\\mathbf{w}}{\\operatorname{arg min}} \\textit{L} (D_{tr} \\cup (\\mathbf{x}_c, y_c), \\mathbf{w})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Where $\\mathbf{x_c}$ is the poisoning point, $A$ is the attacker objective function, $L$ is the classifier training function.\n",
    "Moreover, $D_{tr}$ is the training dataset and $D_{val}$ is the validation dataset.\n",
    "The former problem, along with the poisoning point $\\mathbf{x}_c$ is used to train the classifier on the poisoned data, while the latter is used to evaluate the performance on the untainted data.\n",
    "\n",
    "The former equation depends on the classifier weights, which in turns, depends on the poisoning point.\n",
    "\n",
    "This attack is implemented in SecML by different subclasses of the `CAttackPoisoning`.\n",
    "For the purpose of attacking a SVM classifier we use the `CAttackPoisoningSVM` class.\n",
    "\n",
    "As done for the [evasion attacks](03-Evasion.ipynb), let's specify the parameters first. We set the bounds of the attack space to the known feature space given by validation dataset.\n",
    "Lastly, we chose the solver parameters for this specific optimization problem.\n",
    "\n",
    "Let's start visualizing the objective function considering a single poisoning point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lb, ub = val.X.min(), val.X.max()  # Bounds of the attack space. Can be set to `None` for unbounded\n",
    "\n",
    "# Should be chosen depending on the optimization problem\n",
    "solver_params = {\n",
    "    'eta': 0.05,\n",
    "    'eta_min': 0.05,\n",
    "    'eta_max': None,\n",
    "    'max_iter': 100,\n",
    "    'eps': 1e-6\n",
    "}\n",
    "\n",
    "from secml.adv.attacks import CAttackPoisoningSVM\n",
    "\n",
    "pois_attack = CAttackPoisoningSVM(classifier=clean_clf,\n",
    "                                  training_data=tr,\n",
    "                                  val=val,\n",
    "                                  lb=lb, ub=ub,\n",
    "                                  solver_params=solver_params,\n",
    "                                  random_seed=random_state)\n",
    "\n",
    "# chose and set the initial poisoning sample features and label\n",
    "xc = tr[0, :].X\n",
    "yc = tr[0, :].Y\n",
    "pois_attack.x0 = xc\n",
    "pois_attack.xc = xc\n",
    "pois_attack.yc = yc\n",
    "\n",
    "print(\"Initial poisoning sample features: {:}\".format(xc.ravel()))\n",
    "print(\"Initial poisoning sample label: {:}\".format(yc.item()))\n",
    "\n",
    "from secml.figure import CFigure\n",
    "# Only required for visualization in notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "fig = CFigure(4, 5)\n",
    "\n",
    "grid_limits = [(lb - 0.1, ub + 0.1),\n",
    "               (lb - 0.1, ub + 0.1)]\n",
    "\n",
    "fig.sp.plot_ds(tr)\n",
    "\n",
    "# highlight the initial poisoning sample showing it as a star\n",
    "fig.sp.plot_ds(tr[0, :], markers='*', markersize=16)\n",
    "\n",
    "fig.sp.title('Attacker objective and gradients')\n",
    "fig.sp.plot_fun(\n",
    "    func=pois_attack.objective_function,\n",
    "    grid_limits=grid_limits, plot_levels=False,\n",
    "    n_grid_points=10, colorbar=True)\n",
    "\n",
    "# plot the box constraint\n",
    "from secml.optim.constraints import CConstraintBox\n",
    "\n",
    "box = fbox = CConstraintBox(lb=lb, ub=ub)\n",
    "fig.sp.plot_constraint(box, grid_limits=grid_limits,\n",
    "                       n_grid_points=10)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, we set the desired number of adversarial points to generate, 20 in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "steps = 20\n",
    "n_poisoning_points = steps  # Number of poisoning points to generate\n",
    "pois_attack.n_points = n_poisoning_points\n",
    "\n",
    "# Run the poisoning attack\n",
    "print(\"Attack started...\")\n",
    "pois_y_pred, pois_scores, pois_ds, f_opt = pois_attack.run(ts.X, ts.Y)\n",
    "print(\"Attack complete!\")\n",
    "\n",
    "# Evaluate the accuracy of the original classifier\n",
    "acc = metric.performance_score(y_true=ts.Y, y_pred=y_pred)\n",
    "# Evaluate the accuracy after the poisoning attack\n",
    "pois_acc = metric.performance_score(y_true=ts.Y, y_pred=pois_y_pred)\n",
    "\n",
    "print(\"Original accuracy on test set: {:.2%}\".format(acc))\n",
    "print(\"Accuracy after attack on test set: {:.2%}\".format(pois_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can see that the classifiers has been successfully attacked.\n",
    "To increase the attack power, more poisoning points can be crafted, at the expense of a much slower optimization process.\n",
    "\n",
    "Let's now visualize the attack on a 2D plane.\n",
    "We need to train a copy of the original classifier on the join between the training set and the poisoning points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Training of the poisoned classifier\n",
    "poisoned_clf = clean_clf.deepcopy()\n",
    "pois_tr = tr.append(pois_ds)  # Join the training set with the poisoning points\n",
    "poisoned_clf.fit(pois_tr.X, pois_tr.Y)\n",
    "\n",
    "# Define common bounds for the subplots\n",
    "min_limit = min(pois_tr.X.min(), ts.X.min())\n",
    "max_limit = max(pois_tr.X.max(), ts.X.max())\n",
    "grid_limits = [[min_limit, max_limit], [min_limit, max_limit]]\n",
    "\n",
    "fig = CFigure(10, 10)\n",
    "\n",
    "fig.subplot(2, 2, 1)\n",
    "fig.sp.title(\"Original classifier (training set)\")\n",
    "fig.sp.plot_decision_regions(\n",
    "    clean_clf, n_grid_points=200, grid_limits=grid_limits)\n",
    "fig.sp.plot_ds(tr, markersize=5)\n",
    "fig.sp.grid(grid_on=False)\n",
    "\n",
    "fig.subplot(2, 2, 2)\n",
    "fig.sp.title(\"Poisoned classifier (training set + poisoning points)\")\n",
    "fig.sp.plot_decision_regions(\n",
    "    poisoned_clf, n_grid_points=200, grid_limits=grid_limits)\n",
    "fig.sp.plot_ds(tr, markersize=5)\n",
    "fig.sp.plot_ds(pois_ds, markers=['*', '*'], markersize=12)\n",
    "fig.sp.grid(grid_on=False)\n",
    "\n",
    "fig.subplot(2, 2, 3)\n",
    "fig.sp.title(\"Original classifier (test set)\")\n",
    "fig.sp.plot_decision_regions(\n",
    "    clean_clf, n_grid_points=200, grid_limits=grid_limits)\n",
    "fig.sp.plot_ds(ts, markersize=5)\n",
    "fig.sp.text(0.05, -0.25, \"Accuracy on test set: {:.2%}\".format(acc),\n",
    "            bbox=dict(facecolor='white'))\n",
    "fig.sp.grid(grid_on=False)\n",
    "\n",
    "fig.subplot(2, 2, 4)\n",
    "fig.sp.title(\"Poisoned classifier (test set)\")\n",
    "fig.sp.plot_decision_regions(\n",
    "    poisoned_clf, n_grid_points=200, grid_limits=grid_limits)\n",
    "fig.sp.plot_ds(ts, markersize=5)\n",
    "fig.sp.text(0.05, -0.25, \"Accuracy on test set: {:.2%}\".format(pois_acc),\n",
    "            bbox=dict(facecolor='white'))\n",
    "fig.sp.grid(grid_on=False)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can see how the SVM classifier decision functions *changes* after injecting the adversarial poisoning points (blue and red stars).\n",
    " \n",
    "For more details about poisoning adversarial attacks please refer to:\n",
    "\n",
    "  > [[biggio18-pr]](https://arxiv.org/abs/1712.03141)\n",
    "  > Biggio, B. and Roli, F., 2018. Wild patterns: Ten years after the rise of \n",
    "  > adversarial machine learning. In Pattern Recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Performance of classifier for more poisoning samples\n",
    "\n",
    "Instead of hard-coding the number of points, we can compute poisoning for different fractions of points, as we did for evasion attacks.\n",
    "After each poisoning, we train a classifier with both clean and poisoned samples and we compute the accuracy."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from secml.array import CArray\n",
    "\n",
    "n_poisonous_samples = CArray([1, 10, steps, 75, 100])\n",
    "\n",
    "\n",
    "poisoned_clfs = []\n",
    "test_accuracies = CArray.zeros(n_poisonous_samples.shape[0])\n",
    "\n",
    "for i, p in enumerate(n_poisonous_samples):\n",
    "    print(f'Computing poisoning for {p} points')\n",
    "    pois_attack = ... #Instatiate the poisoning attack as seen before\n",
    "    pois_attack.n_points = p\n",
    "    _, _, ds, _ = pois_attack.run(ts.X, ts.Y)\n",
    "    poisoned_clf = clean_clf.deepcopy()\n",
    "    pois_tr = tr.append(ds)\n",
    "    poisoned_clf.fit(pois_tr.X, pois_tr.Y)\n",
    "    y_pred = ... # compute predictions on the test set using the poisoned model\n",
    "    accuracy = ... # Compute the accuracy on the test set\n",
    "    test_accuracies[i] = accuracy\n",
    "    poisoned_clfs.append(poisoned_clf)\n",
    "    print(f'Test accuracy is {accuracy}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "print(test_accuracies)\n",
    "fig = CFigure()\n",
    "fig.sp.plot(n_poisonous_samples, test_accuracies)\n",
    "fig.sp.xlabel('Poison samples')\n",
    "fig.sp.xticks(n_poisonous_samples)\n",
    "fig.sp.ylabel('Test accuracy')\n",
    "fig.sp.ylim([0,1])\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Robustness of poisoned models\n",
    "\n",
    "Test accuracy is reduced the more samples we inject inside the training set of the classifier.\n",
    "What happens if we attack these poisoned models?\n",
    "What do you expect it will happen?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from secml.adv.seceval import CSecEval\n",
    "from secml.adv.attacks import CFoolboxPGDL2\n",
    "\n",
    "sec_evals = []\n",
    "epsilons = CArray([0.05, 0.1, 0.2, 0.5])\n",
    "fig = CFigure()\n",
    "\n",
    "abs_stepsize = 0.05\n",
    "\n",
    "for p_clf, n in zip(poisoned_clfs, n_poisonous_samples):\n",
    "    print(f'Security evaluation of classifier with {n} poisoned samples...')\n",
    "    attack = ...#  Instantiate a CFoolboxPGDL2 attack\n",
    "    sec_eval = ... # instantiate CSecEval object\n",
    "    # Run the security evaluation as done in the previous laboratory\n",
    "    fig.sp.plot_sec_eval(sec_eval.sec_eval_data, label=f'{n} p')\n",
    "    sec_evals.append(sec_eval)\n",
    "fig.sp.ylim([0,1])\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
